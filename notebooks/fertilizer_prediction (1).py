# -*- coding: utf-8 -*-
"""fertilizer Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EvVj5mx0y39CHLz9i0mYC9M9oQfAfDlu

###Import the libraries
"""

# Importing the necessary libraries

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score
from tabulate import tabulate

"""###Load the dataset
The dataset used in this notebook was sourced from Kaggle and it contains information of varoius Fertilizers.
"""

# Load the dataset

fer = pd.read_csv('/content/Fertilizer Prediction.csv')

"""*ATTRIBUTES DESCRIPTION:*


*   **Temparature -** 	Temperature in degree Celsius
*   **Humidity -** Relative Humdity (%)
*   **Moisture -** Ratio of the mass of water
*   **Soil Type -** Types of Soils
*   **Crop Type -** Type of Crops
*   **Nitrogen -** Amount of Nitrogen in Soil (%)
*   **Potassium -** Amount of Potassium in Soil (%)
*   **Phosphorous -** Amount of Phosphorous in Soil (%)
*   **Fertilizer Name -** Various types of Fertilizers used for different types of Soils & Crops

###Exploratory Data Analysis
"""

fer.head()

fer.describe()

fer.info()

fer.columns

fer['Fertilizer Name'].unique()

# Finding the shape of the dataset

fer.shape

fer.hist(figsize=(12, 8))
plt.tight_layout()
plt.show()

# Bar plot of 'Fertilizer Name'

plt.figure(figsize=(8, 6))
sns.countplot(x='Fertilizer Name', data=fer)
plt.title('Frequency Count of Fertilizer Types')
plt.xticks(rotation=45)
plt.ylim(5, 23)  # Set y-axis limit to ensure all accuracies are visible
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines for better visualization
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()

"""From the above bar plot, there are higher number of instances with 'Urea' which suggests that *urea is the most commonly used fertilizer* in the dataset.

###Data Preprocessing
"""

# Checking for missing values

fer.isnull().sum()

# Encoding categorical column into numerical

from sklearn.preprocessing import LabelEncoder
le= LabelEncoder()
fer['Fertilizer Name']= le.fit_transform(fer['Fertilizer Name'])
fer['Soil Type']= le.fit_transform(fer['Soil Type'])
fer['Crop Type']= le.fit_transform(fer['Crop Type'])

fer

# Extracting independent and dependent variables

x= fer.iloc[:,:-1].values
y= fer.iloc[:,-1].values

"""Auto Data Split"""

# Auto Data Split

def performance(x_train, x_test, y_train, y_test, classifier):
    classifier.fit(x_train, y_train)
    y_pred = classifier.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    return accuracy, precision, recall, f1

def find_optimal_split_ratio(x, y, classifiers, splitting_ratios):
    optimal_ratio = None
    max_accuracy = 0.0
    header = ["Test Ratio", "Classifier", "Accuracy", "Precision", "Recall", "F1-score"]
    table_data = []
    for test_ratio in splitting_ratios:
        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_ratio, random_state=42)
        for classifier_name, classifier in classifiers.items():
            accuracy, precision, recall, f1 = performance(x_train, x_test, y_train, y_test, classifier)
            row_data = [f"{test_ratio:.2f}", classifier_name,
                        f"{accuracy:.4f}", f"{precision:.4f}",
                        f"{recall:.4f}", f"{f1:.4f}"]
            table_data.append(row_data)
            if accuracy > max_accuracy:
                max_accuracy = accuracy
                optimal_ratio = test_ratio
    print("\nOptimal Splitting Ratio:", optimal_ratio)
    print(tabulate(table_data, headers=header, tablefmt="grid"))

# Define the classifiers

classifiers = {'Random Forest': RandomForestClassifier(),
              'Support Vector Machine': SVC(),
              'Decision Tree': DecisionTreeClassifier(),
              'KNN': KNeighborsClassifier()
              }

# Define Splitting ratios to evaluate

splitting_ratios = np.linspace(0.1, 0.9)

#find the optimal splitting ratio

find_optimal_split_ratio(x, y, classifiers, splitting_ratios)

"""###Model Building"""

# Splitting into train and test set

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.19, random_state = 42)

# RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=100, criterion='entropy', max_depth=50, min_samples_split=20, random_state=42)
rfc.fit(x_train, y_train)
y_pred1 = rfc.predict(x_test)

print('Random Forest Classifier\n')
rfc_acc = accuracy_score(y_test, y_pred1)
print(f'Accuracy: {rfc_acc}')
rfc_precision = precision_score(y_test, y_pred1, average='weighted')
print(f'Precision: {rfc_precision}')
rfc_recall = recall_score(y_test, y_pred1, average='weighted')
print(f'Recall: {rfc_recall}')
rfc_f1 = f1_score(y_test, y_pred1, average='weighted')
print(f'F1-score: {rfc_f1}')

# KNeighbors Classifier

knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(x_train, y_train)
y_pred2 = knn.predict(x_test)

print('K-Nearest Neighbor\n')
knn_acc = accuracy_score(y_test, y_pred2)
print(f'Accuracy: {knn_acc}')
knn_precision = precision_score(y_test, y_pred2, average='weighted', zero_division='warn')
print(f'Precision: {knn_precision}')
knn_recall = recall_score(y_test, y_pred2, average='weighted', zero_division='warn')
print(f'Recall: {knn_recall}')
knn_f1 = f1_score(y_test, y_pred2, average='weighted')
print(f'F1-score: {knn_f1}')

# Support Vector Classsifier

svm= SVC(kernel = 'poly', degree = 10, random_state=42)
svm.fit(x_train, y_train)
y_pred3 = svm.predict(x_test)

print('Support Vector Classifier\n')
svm_acc = accuracy_score(y_test, y_pred3)
print(f'Accuracy: {svm_acc}')
svm_precision = precision_score(y_test, y_pred3, average='weighted')
print(f'Precision: {svm_precision}')
svm_recall = recall_score(y_test, y_pred3, average='weighted')
print(f'Recall: {svm_recall}')
svm_f1 = f1_score(y_test, y_pred3, average='weighted')
print(f'F1-score: {svm_f1}')

# Decision Tree Classifier

dt = DecisionTreeClassifier(max_depth = 5, random_state=42)
dt.fit(x_train, y_train)
y_pred4 = dt.predict(x_test)

print('Decision Tree Classifier\n')
dt_acc = accuracy_score(y_test, y_pred4)
print(f'Accuracy: {dt_acc}')
dt_precision = precision_score(y_test, y_pred4, average='weighted', zero_division='warn')
print(f'Precision: {dt_precision}')
dt_recall = recall_score(y_test, y_pred4, average='weighted', zero_division='warn')
print(f'Recall: {dt_recall}')
dt_f1 = f1_score(y_test, y_pred4, average='weighted')
print(f'F1-score: {dt_f1}')

"""*   Random forest Classifier followed with an accuracy of **0.894**

*   K Neighbors Classifier has an accuracy of **0.842**
*   Support Vector Classsifier follows as accuracy of **0.947**
*   Decision Tree Model has an accuracy of **0.894**

The *Support Vector Classifier* showed the highest accuracy of **94%** among the models tested, making it more effective model for predicting fertilizer recommendations for the given dataset.
"""

